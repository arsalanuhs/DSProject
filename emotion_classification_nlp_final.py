# -*- coding: utf-8 -*-
"""Emotion_Classification_NLP_final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zQcRl09gI5hnFYjRSwHXS5qT0twTqfHp
"""

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("AdamCodd/emotion-balanced")

dataset

!pip install datasets transformers torch pandas numpy scikit-learn

import numpy as np
import pandas as pd
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer
import torch
from torch.utils.data import Dataset, DataLoader

# Convert to pandas DataFrames for EDA
train_df = pd.DataFrame(dataset['train'])
valid_df = pd.DataFrame(dataset['validation'])
test_df = pd.DataFrame(dataset['test'])

# Define label mapping
label_map = {
    0: 'sadness',
    1: 'joy',
    2: 'love',
    3: 'anger',
    4: 'fear',
    5: 'surprise'
}

# Add text label column for verification
for df in [train_df, valid_df, test_df]:
    df['emotion'] = df['label'].map(label_map)

# Check class distribution
print("Train Class Distribution:")
print(train_df['emotion'].value_counts())
print("\nValidation Class Distribution:")
print(valid_df['emotion'].value_counts())
print("\nTest Class Distribution:")
print(test_df['emotion'].value_counts())

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Prepare the data for logistic regression
X_train = train_df['text']
y_train = train_df['label']
X_valid = valid_df['text']
y_valid = valid_df['label']

# Use TF-IDF to vectorize the text data
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_valid_vec = vectorizer.transform(X_valid)

# Train a logistic regression model
logreg_model = LogisticRegression(max_iter=1000) # Increased max_iter
logreg_model.fit(X_train_vec, y_train)

# Make predictions on the validation set
y_pred = logreg_model.predict(X_valid_vec)

# Evaluate the model
print(classification_report(y_valid, y_pred))

from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming y_valid and y_pred are already defined from the previous code

# Confusion Matrix
cm = confusion_matrix(y_valid, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_map.values(), yticklabels=label_map.values())
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# ROC AUC Score and Curve (requires probability predictions)
y_prob = logreg_model.predict_proba(X_valid_vec)

# ROC AUC Score for each class (macro-averaged)
roc_auc = roc_auc_score(y_valid, y_prob, multi_class='ovr', average='macro')
print(f"ROC AUC Score (macro-averaged): {roc_auc}")

# ROC curve for each class (optional - can be computationally intensive)
plt.figure(figsize=(8,6))
for i in range(len(label_map)):
  fpr, tpr, _ = roc_curve(np.array(pd.get_dummies(y_valid))[:, i], y_prob[:,i])
  plt.plot(fpr, tpr, label=f'{label_map[i]} (AUC = {roc_auc_score(np.array(pd.get_dummies(y_valid))[:, i], y_prob[:,i]):.2f})')

plt.plot([0,1],[0,1], color='grey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# Preprocessing for Traditional ML Models (TF-IDF)
def prepare_tfidf_features(train_texts, test_texts):
    vectorizer = TfidfVectorizer(max_features=5000)
    X_train = vectorizer.fit_transform(train_texts)
    X_test = vectorizer.transform(test_texts)
    return X_train, X_test, vectorizer

# Prepare TF-IDF features
X_train_tfidf, X_test_tfidf, tfidf_vectorizer = prepare_tfidf_features(
    train_df['text'], test_df['text']
)

# Save TF-IDF features for later use
np.savez('tfidf_features.npz',
         X_train=X_train_tfidf,
         X_test=X_test_tfidf,
         y_train=train_df['label'],
         y_test=test_df['label'])

# Preprocessing for Deep Learning Models (BERT)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

# Tokenize all splits
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Set format for PyTorch
tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

# Create DataLoaders
batch_size = 32

train_loader = DataLoader(
    tokenized_datasets['train'],
    shuffle=True,
    batch_size=batch_size
)

valid_loader = DataLoader(
    tokenized_datasets['validation'],
    batch_size=batch_size
)

test_loader = DataLoader(
    tokenized_datasets['test'],
    batch_size=batch_size
)

# Custom Dataset Class for future use
class EmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label)
        }

# Example usage of custom dataset
train_dataset = EmotionDataset(
    train_df['text'].values,
    train_df['label'].values,
    tokenizer,
    max_length=128
)

# Save processed data for later use
torch.save(tokenized_datasets, 'tokenized_datasets.pt')
torch.save(train_loader, 'train_loader.pt')
torch.save(valid_loader, 'valid_loader.pt')
torch.save(test_loader, 'test_loader.pt')

print("Preprocessing completed successfully!")

# Label mapping
label_map = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}
for df in [train_df, valid_df, test_df]:
    df['emotion'] = df['label'].map(label_map)

# Display dataset info
print("Training set size:", len(train_df))
print("Validation set size:", len(valid_df))
print("Test set size:", len(test_df))

# TF-IDF Vectorization for Traditional ML Models
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(train_df['text'])
X_test_tfidf = tfidf.transform(test_df['text'])

# BERT Tokenization for Deep Learning Models
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=128)
valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=128)

import matplotlib.pyplot as plt
import seaborn as sns

# Class Distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='emotion', data=train_df)
plt.title('Class Distribution in Training Set')
plt.show()

# Text Length Analysis
train_df['text_length'] = train_df['text'].apply(len)
plt.figure(figsize=(10, 6))
sns.histplot(train_df['text_length'], bins=50)
plt.title('Text Length Distribution')
plt.show()

# Traditional ML Models
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# SVM
svm_model = SVC(kernel='linear')
svm_model.fit(X_train_tfidf, train_df['label'])
svm_preds = svm_model.predict(X_test_tfidf)
print("SVM Accuracy:", accuracy_score(test_df['label'], svm_preds))
print(classification_report(test_df['label'], svm_preds, target_names=label_map.values()))

# Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train_tfidf, train_df['label'])
nb_preds = nb_model.predict(X_test_tfidf)
print("Naive Bayes Accuracy:", accuracy_score(test_df['label'], nb_preds))
print(classification_report(test_df['label'], nb_preds, target_names=label_map.values()))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix


# Confusion Matrix for SVM
cm_svm = confusion_matrix(test_df['label'], svm_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svm, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_map.values(), yticklabels=label_map.values())
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix for SVM")
plt.show()

# Confusion Matrix for Naive Bayes
cm_nb = confusion_matrix(test_df['label'], nb_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_nb, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_map.values(), yticklabels=label_map.values())
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix for Naive Bayes")
plt.show()

from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np


# ROC AUC Curve for SVM (using decision function and manually calculating probabilities)
svm_probs = svm_model.decision_function(X_test_tfidf)
# If using a kernel that supports probability=True, use this instead:
#svm_probs = svm_model.predict_proba(X_test_tfidf)
# For a linear kernel, we need to calculate probabilities manually
from scipy.special import softmax # Import softmax
svm_probs = softmax(svm_probs, axis=1) # Apply softmax to get probabilities

roc_auc_svm = roc_auc_score(test_df['label'], svm_probs, multi_class='ovr', average='macro')
print(f"SVM ROC AUC Score (macro-averaged): {roc_auc_svm}")

plt.figure(figsize=(8,6))
for i in range(len(label_map)):
    fpr, tpr, _ = roc_curve(np.array(pd.get_dummies(test_df['label']))[:, i], svm_probs[:,i])
    plt.plot(fpr, tpr, label=f'{label_map[i]} (AUC = {roc_auc_score(np.array(pd.get_dummies(test_df["label"]))[:, i], svm_probs[:,i]):.2f})')

plt.plot([0,1],[0,1], color='grey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for SVM')
plt.legend()
plt.show()

# ROC AUC Curve for Naive Bayes
nb_probs = nb_model.predict_proba(X_test_tfidf)
roc_auc_nb = roc_auc_score(test_df['label'], nb_probs, multi_class='ovr', average='macro')
print(f"Naive Bayes ROC AUC Score (macro-averaged): {roc_auc_nb}")

plt.figure(figsize=(8,6))
for i in range(len(label_map)):
    fpr, tpr, _ = roc_curve(np.array(pd.get_dummies(test_df['label']))[:, i], nb_probs[:,i])
    plt.plot(fpr, tpr, label=f'{label_map[i]} (AUC = {roc_auc_score(np.array(pd.get_dummies(test_df["label"]))[:, i], nb_probs[:,i]):.2f})')

plt.plot([0,1],[0,1], color='grey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Naive Bayes')
plt.legend()
plt.show()

!pip install --upgrade transformers

# Preprocessing for Deep Learning Models (BERT)

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer # Import TrainingArguments
import torch

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')  # Using DistilBERT


def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

# Tokenize all splits
train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=128)
valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, max_length=128)

# Convert encodings to PyTorch datasets
class EmotionDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}  # Removed .to(self.device)
        item['labels'] = torch.tensor(self.labels[idx])  # Removed .to(self.device)
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = EmotionDataset(train_encodings, train_df['label'].tolist())
valid_dataset = EmotionDataset(valid_encodings, valid_df['label'].tolist())

# Check for GPU availability
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using GPU:", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print("Using CPU")

# Load pre-trained BERT model with a smaller architecture
model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6).to(device)

# Define Trainer with optimized parameters
training_args = TrainingArguments(
    output_dir='./results',
    eval_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=4,  # Further reduced batch size
    num_train_epochs=1,  # Reduced epochs
    weight_decay=0.01,
    fp16=True,
    gradient_accumulation_steps=4,  # Increased accumulation steps
    report_to="none",  # Disable wandb reporting
)

# Create Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
)

# Fine-tune BERT
trainer.train()

print("Fine-tuning completed!")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve
import numpy as np
import pandas as pd
from scipy.special import softmax



# Predictions from the fine-tuned model (replace with actual predictions)
predictions = trainer.predict(valid_dataset)
predicted_labels = np.argmax(predictions.predictions, axis=1)

# Confusion Matrix for DistilBERT
cm_distilbert = confusion_matrix(valid_df['label'], predicted_labels)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_distilbert, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_map.values(), yticklabels=label_map.values())
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix for DistilBERT")
plt.show()


# ROC AUC Curve for DistilBERT
distilbert_probs = softmax(predictions.predictions, axis=1) # Apply softmax
roc_auc_distilbert = roc_auc_score(valid_df['label'], distilbert_probs, multi_class='ovr', average='macro')
print(f"DistilBERT ROC AUC Score (macro-averaged): {roc_auc_distilbert}")

plt.figure(figsize=(8,6))
for i in range(len(label_map)):
    fpr, tpr, _ = roc_curve(np.array(pd.get_dummies(valid_df['label']))[:, i], distilbert_probs[:,i])
    plt.plot(fpr, tpr, label=f'{label_map[i]} (AUC = {roc_auc_score(np.array(pd.get_dummies(valid_df["label"]))[:, i], distilbert_probs[:,i]):.2f})')

plt.plot([0,1],[0,1], color='grey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for DistilBERT')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

# Assuming predicted_labels and valid_df['label'] are defined as in the previous code

# Calculate metrics
accuracy = accuracy_score(valid_df['label'], predicted_labels)
f1 = f1_score(valid_df['label'], predicted_labels, average='weighted')  # Use weighted average for multi-class
recall = recall_score(valid_df['label'], predicted_labels, average='weighted')
precision = precision_score(valid_df['label'], predicted_labels, average='weighted')

# Print the results
print(f"DistilBERT Accuracy: {accuracy}")
print(f"DistilBERT F1 Score: {f1}")
print(f"DistilBERT Recall: {recall}")
print(f"DistilBERT Precision: {precision}")

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Get predictions for the validation set
predictions = trainer.predict(valid_dataset)
predicted_labels = np.argmax(predictions.predictions, axis=1)
true_labels = predictions.label_ids

# Calculate metrics
accuracy = accuracy_score(true_labels, predicted_labels)
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')

# Print metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Confusion Matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_map.values(), yticklabels=label_map.values())
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

import torch
import numpy as np

def predict_emotion(text, model, tokenizer, label_map):
    """Predicts the emotion of a given text using the fine-tuned BERT model.

    Args:
        text (str): The input text.
        model: The fine-tuned BERT model.
        tokenizer: The tokenizer used for the model.
        label_map (dict): A dictionary mapping label indices to emotion names.

    Returns:
        str: The predicted emotion.
    """
    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

    # Move inputs to the device (GPU if available, otherwise CPU)
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Make prediction
    with torch.no_grad():
        outputs = model(**inputs)
        predicted_class_id = np.argmax(outputs.logits.cpu().numpy()).flatten().item()

    # Get emotion label
    predicted_emotion = label_map[predicted_class_id]

    return predicted_emotion

# Get user input
user_input = input("Enter text: ")

# Predict emotion
predicted_emotion = predict_emotion(user_input, model, tokenizer, label_map)

# Print the prediction
print(f"Predicted Emotion: {predicted_emotion}")